{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "sem4_semantics.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wC97sL7DMqwj"
      },
      "source": [
        "## Задание"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrVu2eJmMqwl"
      },
      "source": [
        "Реализуйте поиск по нашему стандартному Covid корпусу с помощью модели на Araneum двумя способами:\n",
        "\n",
        "    1. преобразуйте каждый документ в вектор через усреднение векторов его слов и реализуйте поисковик как \n",
        "    обычно через умножение матрицы документов коллекции на вектор запроса \n",
        "    2. экспериментальный способ - реализуйте поиск ближайшего документа в коллекции к запросу, преобразовав \n",
        "    каждый документ в матрицу (количество слов x размер модели)\n",
        "    \n",
        "Посчитайте качество поиска для каждой модели на тех же данных, что и в предыдущем задании. В качестве препроцессинга используйте две версии - с удалением NER и без удаления.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmxTblei1bkg"
      },
      "source": [
        "Установка необходимых библиотек:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwzvkX_2Mqwp",
        "outputId": "4c35ed68-66f4-4ad7-cede-38f5e1e6eaa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "!pip install gensim --upgrade"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/e0/fa6326251692056dc880a64eb22117e03269906ba55a6864864d24ec8b4e/gensim-3.8.3-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2MB 1.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (2.2.0)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-3.8.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gensim"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uk-kVxenhUAp",
        "outputId": "d6c47d90-a412-41c7-fb5e-73df574baeee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        }
      },
      "source": [
        "!pip install natasha"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting natasha\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/34/9abb6b5c95993001518e517f21157e2c955749ac4f3c79dc3c2cf25e72fe/natasha-1.3.0-py3-none-any.whl (34.4MB)\n",
            "\u001b[K     |████████████████████████████████| 34.4MB 112kB/s \n",
            "\u001b[?25hCollecting ipymarkup>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bf/9b/bf54c98d50735a4a7c84c71e92c5361730c878ebfe903d2c2d196ef66055/ipymarkup-0.9.0-py3-none-any.whl\n",
            "Collecting pymorphy2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/57/b2ff2fae3376d4f3c697b9886b64a54b476e1a332c67eee9f88e7f1ae8c9/pymorphy2-0.9.1-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.8MB/s \n",
            "\u001b[?25hCollecting razdel>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/15/2c/664223a3924aa6e70479f7d37220b3a658765b9cfe760b4af7ffdc50d38f/razdel-0.5.0-py3-none-any.whl\n",
            "Collecting yargy>=0.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/07/94306844e3a5cb520660612ad98bce56c168edb596679bd541e68dfde089/yargy-0.14.0-py3-none-any.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.4MB/s \n",
            "\u001b[?25hCollecting slovnet>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/6f/1c989335c9969421f771e4f0410ba70d82fe992ec9f3cbac9f432d8f5733/slovnet-0.4.0-py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.7MB/s \n",
            "\u001b[?25hCollecting navec>=0.9.0\n",
            "  Downloading https://files.pythonhosted.org/packages/83/ad/554945ebee66fe83fefd61e043938981dd9e6136882025c506ac6faa6a4c/navec-0.9.0-py3-none-any.whl\n",
            "Collecting intervaltree>=3\n",
            "  Downloading https://files.pythonhosted.org/packages/50/fb/396d568039d21344639db96d940d40eb62befe704ef849b27949ded5c3bb/intervaltree-3.1.0.tar.gz\n",
            "Collecting dawg-python>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/79/bea0021eeb7eeefde22ef9e96badf174068a2dd20264b9a378f2be1cdd9e/pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2MB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2->natasha) (0.6.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from slovnet>=0.3.0->natasha) (1.18.5)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.2.2)\n",
            "Building wheels for collected packages: intervaltree\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26100 sha256=743b6330fa9f9411133a71c5eeb3db8118a17ff065758f3486c7f7050a754ac2\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/f2/66/e9c30d3e9499e65ea2fa0d07c002e64de63bd0adaa49c445bf\n",
            "Successfully built intervaltree\n",
            "Installing collected packages: intervaltree, ipymarkup, dawg-python, pymorphy2-dicts-ru, pymorphy2, razdel, yargy, navec, slovnet, natasha\n",
            "  Found existing installation: intervaltree 2.1.0\n",
            "    Uninstalling intervaltree-2.1.0:\n",
            "      Successfully uninstalled intervaltree-2.1.0\n",
            "Successfully installed dawg-python-0.7.2 intervaltree-3.1.0 ipymarkup-0.9.0 natasha-1.3.0 navec-0.9.0 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 razdel-0.5.0 slovnet-0.4.0 yargy-0.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsRNuPnN1la-"
      },
      "source": [
        "Скачиваем с **rusvectores** модель araneum_none_fasttextcbow_300_5_2018 (fasttext) и разархивируем ее"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BldQ5cldMSc",
        "outputId": "0f10d3e9-20e0-49e8-8388-93f2c47f61f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "!wget https://rusvectores.org/static/models/rusvectores4/fasttext/araneum_none_fasttextcbow_300_5_2018.tgz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-21 00:00:35--  https://rusvectores.org/static/models/rusvectores4/fasttext/araneum_none_fasttextcbow_300_5_2018.tgz\n",
            "Resolving rusvectores.org (rusvectores.org)... 116.203.104.23\n",
            "Connecting to rusvectores.org (rusvectores.org)|116.203.104.23|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2691248108 (2.5G) [application/x-gzip]\n",
            "Saving to: ‘araneum_none_fasttextcbow_300_5_2018.tgz’\n",
            "\n",
            "araneum_none_fastte 100%[===================>]   2.51G  11.3MB/s    in 3m 55s  \n",
            "\n",
            "2020-10-21 00:04:32 (10.9 MB/s) - ‘araneum_none_fasttextcbow_300_5_2018.tgz’ saved [2691248108/2691248108]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNAf77VFezF2"
      },
      "source": [
        "!tar -xf araneum_none_fasttextcbow_300_5_2018.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svQCjpYV11PG"
      },
      "source": [
        "Импорт библиотек:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xXVVfTIgRa8"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import pymorphy2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from math import log\n",
        "from razdel import tokenize\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from natasha import NewsNERTagger, NewsEmbedding, Doc, Segmenter"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsi6uTJY13Ec"
      },
      "source": [
        "Загружаем модель fasttext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYGd678NgmRs"
      },
      "source": [
        "model_file = 'araneum_none_fasttextcbow_300_5_2018.model'\n",
        "model = KeyedVectors.load(model_file)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RO_vG5T2C08"
      },
      "source": [
        "Открываем данные с вопросами по COVID-19"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycuq5spOgtzB"
      },
      "source": [
        "answers_df = pd.read_excel('answers_base.xlsx')\n",
        "queries_df = pd.read_excel('queries_base.xlsx')\n",
        "#в датафрейме с запросами есть пропуски, убираем их\n",
        "queries_df = queries_df[['Текст вопроса', 'Номер связки\\n']].dropna()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q-3XIfo2INF"
      },
      "source": [
        "Делим выборку на тренировочную (все answers и 70% queries) и тестовую (30% queries)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPwPUlMZg2NH"
      },
      "source": [
        "queries_train, queries_test = train_test_split(queries_df, test_size=0.3, random_state=0)\n",
        "documents = answers_df['Текст вопросов'].append(queries_train['Текст вопроса'], ignore_index=True)\n",
        "answers_train = answers_df['Номер связки'].append(queries_train['Номер связки\\n'], ignore_index=True)\n",
        "answers_test = queries_test['Номер связки\\n']"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyQkRymt2UxE"
      },
      "source": [
        "Загружаем необходимые модули для предобработки текста"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UAwiIsthEY2",
        "outputId": "e71e65e0-243b-4364-eff9-37b0f1a3f4b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "nltk.download('stopwords')\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "stopwords = set(stopwords.words('russian'))\n",
        "punkt = punctuation + '«»—…“”*№–'\n",
        "\n",
        "#сущности из natasha\n",
        "emb = NewsEmbedding()\n",
        "segmenter = Segmenter()\n",
        "ner_tagger = NewsNERTagger(emb)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yimE9oh3hnk7"
      },
      "source": [
        "def tokenizing(text):\n",
        "  \"\"\"Функция, делящая текст на токены\"\"\"\n",
        "  tokens = list(tokenize(text))\n",
        "  return [_.text.lower() for _ in tokens]\n",
        "\n",
        "def preprocessing(text):\n",
        "  \"\"\"Основная предобработка: удаление стоп-слов, запятых и лемматизация\"\"\"\n",
        "    tokens = tokenizing(text)\n",
        "    clean_text = [x for x in tokens if x not in stopwords and x not in punkt]\n",
        "    clean_text = [morph.parse(word)[0].normal_form for word in clean_text if re.match('\\W+', word) is None]\n",
        "    return clean_text\n",
        "\n",
        "def preprocess_with_natasha(text: str) -> str:\n",
        "  \"\"\"Удаление именованных сущностей \"\"\"\n",
        "    doc = Doc(text)\n",
        "    doc.segment(segmenter)\n",
        "    doc.tag_ner(ner_tagger)\n",
        "    new_text = text\n",
        "    for entity in doc.spans:\n",
        "        new_text = new_text.replace(text[entity.start:entity.stop], '')\n",
        "    return new_text"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUpZd3yN2oxB"
      },
      "source": [
        "Я выбрала именно наташу, потому что в прошлой домашке она показала себя лучше чем deeppavlov!\n",
        "\n",
        "Применим функции предобработки к текстам:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS_zsM8_iTQv"
      },
      "source": [
        "prep_documents = documents.apply(preprocessing)\n",
        "test_queries = queries_test['Текст вопроса'].apply(preprocessing)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2TX5fvjjCSU"
      },
      "source": [
        "ner_documents = documents.apply(lambda x: preprocessing(preprocess_with_natasha(x)))\n",
        "ner_test_queries = queries_test['Текст вопроса'].apply(lambda x: preprocessing(preprocess_with_natasha(x)))"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgXhoBHLn0rB"
      },
      "source": [
        "## 1. Преобразование каждого документа в вектор через усреднение"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teFuW-aTliRg"
      },
      "source": [
        "def normalize_vec(v):\n",
        "  \"\"\" Функция для нормализации вектора\"\"\"\n",
        "     return v / np.sqrt(np.sum(v ** 2))\n",
        "\n",
        "\n",
        "def text_to_vec(document, dim=model.vector_size):\n",
        "  \"\"\" Функция, переводящая текст документа в вектор через усреднение векторов слов\"\"\"\n",
        "    all_embs = []\n",
        "    for word in document:\n",
        "      # если слово в модели, добавляем в список его вектор\n",
        "      if word in model:\n",
        "        all_embs.append(normalize_vec(model[word]))\n",
        "      #если ни одного слова из документа нет в модели, возвращаем нули\n",
        "    if len(all_embs) == 0:\n",
        "      return np.zeros(dim)\n",
        "    return np.mean(all_embs,axis=0)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NaXtYRK3NDZ"
      },
      "source": [
        "Векторизуем каждый документ и запрос коллекции:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6kMgBp0nqCs"
      },
      "source": [
        "vec_documents = prep_documents.apply(text_to_vec)\n",
        "vec_queries = test_queries.apply(text_to_vec)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPtEcu_YoYo-"
      },
      "source": [
        "vec_documents_ner = ner_documents.apply(text_to_vec)\n",
        "vec_queries_ner = ner_test_queries.apply(text_to_vec)"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGSlGfmy3SBX"
      },
      "source": [
        "Мы получили список, каждый элемент которого - numpy array, а хочется матрицу. Поэтому применим функцию numpy.vstack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4Fls_hOsq-a"
      },
      "source": [
        "vec_documents = np.vstack(vec_documents)\n",
        "vec_queries = np.vstack(vec_queries)\n",
        "\n",
        "vec_documents_ner = np.vstack(vec_documents_ner)\n",
        "vec_queries_ner = np.vstack(vec_queries)"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uULdzb1I31Bu"
      },
      "source": [
        "Приступаем к ранжированию:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHwEOZ7soeqh"
      },
      "source": [
        "def ranging(query, vectors):\n",
        "  \"\"\"Функция, возвращающая топ1 ответ запросу query по косинусной близости векторов\"\"\"\n",
        "  sims = enumerate(vectors.dot(query))\n",
        "  sorted_docs = sorted(sims, key=lambda x: x[1], reverse=True)\n",
        "  top1_doc = sorted_docs[0][0]\n",
        "  return answers_train[top1_doc]"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtPr4Qrw3vie"
      },
      "source": [
        "Для оценивания результатов напишем функцию, считающую долю верно угаданных ответов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NPV53l4qXTH"
      },
      "source": [
        "def accuracy(predicted, answers_test=answers_test):\n",
        "  \"\"\"Функция, которая считает метрику accuracy для полученных ответов\"\"\"\n",
        "  right_answers = answers_test[answers_test==predicted].shape[0]\n",
        "  return right_answers/answers_test.shape[0]"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kv4BiorF34YW"
      },
      "source": [
        "Найдем топ1 нужный ответ для каждого запроса в коллекции"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PG7REVT-vgJg"
      },
      "source": [
        "predicted = [ranging(q, vec_documents) for q in vec_queries]\n",
        "predicted_ner = [ranging(q, vec_documents_ner) for q in vec_queries_ner]"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLAdMOGA38QO"
      },
      "source": [
        "Померяем качество:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gw2t1SMuyvp",
        "outputId": "cde9dc41-180c-4474-e193-6b84297bbdb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print('Качество поиска через умножение матрицы на вектор: ', accuracy(predicted))\n",
        "print('Качество поиска через умножение матрицы на вектор с удалением NER', accuracy(predicted_ner))"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Качество поиска через умножение матрицы на вектор:  0.16739446870451238\n",
            "Качество поиска через умножение матрицы на вектор с удалением NER 0.19068413391557495\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_bjLRilyPyI"
      },
      "source": [
        "## 2. Эксперимент - представление документа в виде матрицы\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oemU3SMq4c9K"
      },
      "source": [
        "У нас уже есть предобработанные тексты, и мы сразу их будем представлять в другом виде:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JReII_yvyV-W"
      },
      "source": [
        "def create_doc_matrix(text, dim=model.vector_size):\n",
        "  \"\"\"Функция с семинара, создающая из документа матрицу векторов его слов\"\"\"\n",
        "    lemmas_vectors = np.zeros((len(text), dim))\n",
        "    vec = np.zeros((dim,))\n",
        "\n",
        "    for idx, lemma in enumerate(text):\n",
        "        if lemma in model:\n",
        "            lemmas_vectors[idx] = normalize_vec(model[lemma])\n",
        "            \n",
        "    return lemmas_vectors  "
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUjMV5HX4Vdx"
      },
      "source": [
        "Применяем функцию выше ко всем запросам и документам"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSfRFRQly1jX"
      },
      "source": [
        "matrix_docs = prep_documents.apply(create_doc_matrix)\n",
        "matrix_queries = test_queries.apply(create_doc_matrix)\n",
        "\n",
        "matrix_docs_ner = ner_documents.apply(create_doc_matrix)\n",
        "matrix_queries_ner = ner_test_queries.apply(create_doc_matrix)"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MJlrD0tzjGW"
      },
      "source": [
        "def search(docs, query, reduce_func=np.max, axis=0):\n",
        "  \"\"\"Функция, возвращающая топ1 ответ для query по умножению матрицы на матрицу (чуть-чуть изменена из семинара)\"\"\"\n",
        "    sims = []\n",
        "    for doc in docs:\n",
        "        sim = doc.dot(query.T)\n",
        "        sim = reduce_func(sim, axis=axis)\n",
        "        sims.append(sim.sum())\n",
        "    #  сортируем полученные значения близостей\n",
        "    sims = enumerate(sims)\n",
        "    sorted_docs = sorted(sims, key=lambda x: x[1], reverse=True)\n",
        "    top1_doc = sorted_docs[0][0]\n",
        "    return answers_train[top1_doc]"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEvpTITW41Op"
      },
      "source": [
        "Находим топ1 для каждого запроса и меряем качество:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3P4RZY5E0aZO"
      },
      "source": [
        "predicted = [search(matrix_docs, q) for q in matrix_queries]\n",
        "predicted_ner = [search(matrix_docs_ner, q) for q in matrix_queries_ner]"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G_iACWe0uwW",
        "outputId": "e13c29e6-44ad-48e8-e405-a87b91eec3dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print('Качество поиска в экспериментальном методе перемножения двух матриц: ', accuracy(predicted))\n",
        "print('Качество поиска в экспериментальном методе перемножения двух матриц с удалением NER', accuracy(predicted_ner))"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Качество поиска в экспериментальном методе перемножения двух матриц:  0.44395924308588064\n",
            "Качество поиска в экспериментальном методе перемножения двух матриц с удалением NER 0.43377001455604075\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reztBDt646XC"
      },
      "source": [
        "Этот подход показал себя намного лучше, чем предыдущий!"
      ]
    }
  ]
}